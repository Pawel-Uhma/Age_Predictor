{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46377e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# If you need to install packages, uncomment these lines:\n",
    "# %pip install torch torchvision pillow matplotlib\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- User-configurable paths ----\n",
    "CKPT_PATH = Path(\"./src/outputs/models/best_resnet50.pth\")\n",
    "IMAGES_DIR = Path(\"./data/inference\")\n",
    "OUTPUT_DIR = Path(\"./data/inference_annotated\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Inference image size\n",
    "TARGET_SIZE = (64, 64)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255e80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_pad_to_size(img: Image.Image, target_size=(64, 64), fill=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Resize keeping aspect ratio and pad to target_size (w, h).\n",
    "    Returns a PIL.Image of exactly target_size.\n",
    "    \"\"\"\n",
    "    tw, th = target_size\n",
    "    w, h = img.size\n",
    "\n",
    "    # Compute scale while preserving aspect ratio\n",
    "    scale = min(tw / w, th / h)\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "\n",
    "    # Resize\n",
    "    img_resized = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "\n",
    "    # Create background and paste centered\n",
    "    background = Image.new(\"RGB\", (tw, th), fill)\n",
    "    paste_x = (tw - new_w) // 2\n",
    "    paste_y = (th - new_h) // 2\n",
    "    background.paste(img_resized, (paste_x, paste_y))\n",
    "    return background\n",
    "\n",
    "# Normalization (adjust if your training used different stats)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Lambda(lambda im: im.convert(\"RGB\")),  # ensure 3 channels\n",
    "    transforms.Lambda(lambda im: letterbox_pad_to_size(im, TARGET_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffade6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_resnet50_from_ckpt(ckpt_path: Path, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load a ResNet-50 model from a checkpoint. Handles common patterns:\n",
    "      - torch.save(model.state_dict())\n",
    "      - torch.save({'state_dict': ..., 'class_to_idx': ...})\n",
    "      - torch.save({'model_state_dict': ..., ...})\n",
    "      - torch.save(model)  # full model (pickled)\n",
    "    Returns (model, idx_to_class) where idx_to_class may be None if unknown.\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    state_dict = None\n",
    "    idx_to_class = None\n",
    "\n",
    "    if isinstance(ckpt, dict):\n",
    "        # Common keys for state dicts\n",
    "        if 'state_dict' in ckpt and isinstance(ckpt['state_dict'], dict):\n",
    "            state_dict = ckpt['state_dict']\n",
    "        elif 'model_state_dict' in ckpt and isinstance(ckpt['model_state_dict'], dict):\n",
    "            state_dict = ckpt['model_state_dict']\n",
    "\n",
    "        # Class mapping if present\n",
    "        if 'idx_to_class' in ckpt and isinstance(ckpt['idx_to_class'], (list, dict)):\n",
    "            if isinstance(ckpt['idx_to_class'], list):\n",
    "                idx_to_class = {i: name for i, name in enumerate(ckpt['idx_to_class'])}\n",
    "            else:\n",
    "                idx_to_class = {int(k): v for k, v in ckpt['idx_to_class'].items()}\n",
    "        elif 'class_to_idx' in ckpt and isinstance(ckpt['class_to_idx'], dict):\n",
    "            class_to_idx = ckpt['class_to_idx']\n",
    "            idx_to_class = {int(v): k for k, v in class_to_idx.items()}\n",
    "\n",
    "    # If checkpoint is a full model object (pickled)\n",
    "    if state_dict is None and hasattr(ckpt, 'state_dict') and callable(ckpt.state_dict):\n",
    "        try:\n",
    "            model = ckpt.to(device)\n",
    "            model.eval()\n",
    "            # Optional mapping on the model\n",
    "            if hasattr(model, 'idx_to_class'):\n",
    "                ic = getattr(model, 'idx_to_class')\n",
    "                if isinstance(ic, list):\n",
    "                    idx_to_class = {i: name for i, name in enumerate(ic)}\n",
    "                elif isinstance(ic, dict):\n",
    "                    idx_to_class = {int(k): v for k, v in ic.items()}\n",
    "            return model, idx_to_class\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to use checkpoint as a full model: {e}\")\n",
    "\n",
    "    # Build a base resnet50\n",
    "    model = models.resnet50(weights=None)\n",
    "    in_features = model.fc.in_features\n",
    "\n",
    "    # Try to infer num_classes from state_dict\n",
    "    num_classes = None\n",
    "    if state_dict is not None:\n",
    "        for k in ['fc.weight', 'module.fc.weight']:\n",
    "            if k in state_dict:\n",
    "                num_classes = state_dict[k].shape[0]\n",
    "                break\n",
    "    if num_classes is None:\n",
    "        num_classes = 1000  # fallback (ImageNet)\n",
    "        print(\"[Warning] Could not infer num_classes from checkpoint. Defaulting to 1000. Adjust if needed.\")\n",
    "\n",
    "    # Replace head\n",
    "    model.fc = nn.Linear(in_features, num_classes, bias=True)\n",
    "\n",
    "    # Remove 'module.' prefix if from DataParallel\n",
    "    def strip_module(sd):\n",
    "        if not any(k.startswith('module.') for k in sd.keys()):\n",
    "            return sd\n",
    "        return {k.replace('module.', '', 1): v for k, v in sd.items()}\n",
    "\n",
    "    if state_dict is not None:\n",
    "        state_dict = strip_module(state_dict)\n",
    "        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "        if missing:\n",
    "            print(\"Missing keys:\", missing)\n",
    "        if unexpected:\n",
    "            print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "    model.to(device).eval()\n",
    "    return model, idx_to_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e0336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths(root: Path):\n",
    "    exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
    "    return sorted([p for p in root.rglob('*') if p.suffix.lower() in exts])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits(model, batch):\n",
    "    return model(batch)\n",
    "\n",
    "def softmax_probs(logits):\n",
    "    return torch.softmax(logits, dim=1)\n",
    "\n",
    "def get_label(idx, idx_to_class):\n",
    "    if idx_to_class and idx in idx_to_class:\n",
    "        return str(idx_to_class[idx])\n",
    "    return str(idx)\n",
    "\n",
    "def draw_label_on_image(img: Image.Image, text: str, margin=6):\n",
    "    \"\"\"\n",
    "    Draw a solid rectangle and white text at top-left of the image.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.load_default()\n",
    "    except Exception:\n",
    "        font = None\n",
    "\n",
    "    # Robust text size\n",
    "    if hasattr(draw, 'textbbox'):\n",
    "        left, top, right, bottom = draw.textbbox((0,0), text, font=font)\n",
    "        text_w, text_h = right - left, bottom - top\n",
    "    else:\n",
    "        text_w, text_h = draw.textsize(text, font=font)\n",
    "\n",
    "    pad = 4\n",
    "    rect_w = text_w + 2 * pad\n",
    "    rect_h = text_h + 2 * pad\n",
    "\n",
    "    draw.rectangle([margin, margin, margin + rect_w, margin + rect_h], fill=(0, 0, 0))\n",
    "    draw.text((margin + pad, margin + pad), text, fill=(255, 255, 255), font=font)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ffbd75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Checkpoint not found at src\\outputs\\best_resnet50.pth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m CKPT_PATH\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCKPT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m model, idx_to_class \u001b[38;5;241m=\u001b[39m load_model_resnet50_from_ckpt(CKPT_PATH, device)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Collect images\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Checkpoint not found at src\\outputs\\best_resnet50.pth"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "assert CKPT_PATH.exists(), f\"Checkpoint not found at {CKPT_PATH}\"\n",
    "model, idx_to_class = load_model_resnet50_from_ckpt(CKPT_PATH, device)\n",
    "\n",
    "# Collect images\n",
    "image_paths = load_image_paths(IMAGES_DIR)\n",
    "print(f\"Found {len(image_paths)} image(s) in {IMAGES_DIR}\")\n",
    "\n",
    "# Inference loop\n",
    "for img_path in image_paths:\n",
    "    orig = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Preprocess to 64x64 with aspect-ratio padding\n",
    "    x = preprocess(orig)                # 3x64x64\n",
    "    x = x.unsqueeze(0).to(device)       # 1x3x64x64\n",
    "\n",
    "    # Forward pass\n",
    "    logits = predict_logits(model, x)\n",
    "    probs = softmax_probs(logits).squeeze(0).cpu()\n",
    "\n",
    "    # Top-1 prediction\n",
    "    top1_prob, top1_idx = torch.max(probs, dim=0)\n",
    "    label = get_label(int(top1_idx.item()), idx_to_class)\n",
    "    conf = float(top1_prob.item())\n",
    "\n",
    "    # Annotate original image for display/saving\n",
    "    annotated = orig.copy()\n",
    "    draw_label_on_image(annotated, f\"{label} ({conf:.2f})\")\n",
    "\n",
    "    # Save annotated copy\n",
    "    out_path = OUTPUT_DIR / img_path.name\n",
    "    annotated.save(out_path)\n",
    "\n",
    "    # Display inline\n",
    "    plt.figure()\n",
    "    plt.imshow(annotated)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{img_path.name} → {label} ({conf:.2f})\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc963ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import ceil\n",
    "saved = sorted([p for p in OUTPUT_DIR.glob('*') if p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}])\n",
    "n = len(saved)\n",
    "if n:\n",
    "    cols = min(4, n)\n",
    "    rows = math.ceil(n / cols)\n",
    "    plt.figure(figsize=(4*cols, 4*rows))\n",
    "    for i, p in enumerate(saved, 1):\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(p.name, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
